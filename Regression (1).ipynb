{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Regression**"
      ],
      "metadata": {
        "id": "x0HTz4HFZnq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment Questions**\n",
        "\n",
        "#Q.1 What is Simple Linear Regression?\n",
        "-> Simple Linear Regression is a statistical method that models the relationship between a dependent variable (Y) and a single independent variable (X) using a linear equation.\n",
        "\n",
        "#Q.2 What are the key assumptions of Simple Linear Regression?\n",
        "-> The key assumptions include linearity, independence, homoscedasticity, normality of residuals, and no multicollinearity.\n",
        "\n",
        "#Q.3 What does the cofficient m represent in the equation y = mX +c?\n",
        "-> The coefficient m represents the slope of the line, indicating the chnage in the dependent variable(Y) for a one-unit change in the independent variable(X).\n",
        "\n",
        "#Q.4 What does the intercept c represent in the equation y = mX +c ?\n",
        "-> The intercept c represents the value of Y when X is zero.\n",
        "\n",
        "#Q.5 How do we calculate the slope m in simple Linear Regression?\n",
        "-> The slope m is calculated using the formula:\n",
        "\n",
        "$$\n",
        "m = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}\n",
        "{ \\sum (X_i - \\bar{X})^2}\n",
        "$$\n",
        "In this formula :    \n",
        "- $X_i and Y_i$ are the individual data points.\n",
        "- $\\bar{X} and \\bar {Y}$ are the means of the X and Y variables, respectively.\n",
        "- The numerator represents the covariance between X and Y.\n",
        "- The denominator represents the variance of X.\n",
        "\n",
        "#Q.6 What is the purpose of the least squares method in simple Linear Regression?\n",
        "-> The least squares method minimizes the sum of the squared differences between observed and predicted values to find the best- fitting line.\n",
        "\n",
        "#Q.7 How is the cofficient of determination(R-square) interpreted in Simple Linear Regression?\n",
        "-> R-square measures the proportion of variance in the dependent variable that is predictable from the independent variable. It ranges from 0 to 1, with higher values indicating a better fit.\n",
        "\n",
        "#Q.8 What is Multiple Linear Regression?\n",
        "-> Multiple Linear Regression models the relationship between a dependent variable and two or more independent variables.\n",
        "\n",
        "#Q.9 What is the main difference between Simple and Multiple Linear Regression?\n",
        "-> Simple Linear Regression uses one independent variable, while Multiple Linear Regression  uses multiple independent variables.\n",
        "\n",
        "#Q.10 What are the key assumptions of Multiple Linear Regression?\n",
        "-> The assumptions include linearity, independence, homoscedasticity, normality of residuals, no multicollinearity, and no autocorrelation.\n",
        "\n",
        "#Q.11 What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "-> Heteroscedasticity is the presence of non- constant variance in residuals. It can lead to inefficient estimates and biased standard errors.\n",
        "\n",
        "#Q.12 How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "-> Techniques include removing correlated predictors, combining predictors, or using regularization methods like Ridge Regression.\n",
        "\n",
        "#Q.13 What are some common techniques for transforming categorical variables for use in regression models?\n",
        "-> Common techniques include one-hot encoding, label encoding, and using dummy variables.\n",
        "\n",
        "#Q.14 What is the role of interaction terms in Multiple Linear Regression?\n",
        "-> Interaction terms allow the effect of one independent variable on the dependent variable to depend on the value of another independent variable.\n",
        "\n",
        "#Q.15 How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "-> In Simple Linear Regression, the intercept is the expected value of Y when X is zero. In Multiple Linear Regression, it is the expected value of Y when all independent variables are zero.\n",
        "\n",
        "#Q.16 What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "-> The slope indicates the strength and direction of the relationship between variables. It affects predictions by determining how much Y changes with a change in X.\n",
        "\n",
        "#Q.17 How does the intercept in a regression model provide context for the relationship between variables?\n",
        "-> The intercept provides a baseline value for the dependent variable when all independent variables are zero,offering context for the starting point of the relationship.\n",
        "\n",
        "#Q.18 What are the limitations of using R-square as a sole measure of model performance?\n",
        "-> R-square does not indicate the correctness of the model, can be inflated by adding more variables, and does not account for overfitting.\n",
        "\n",
        "#Q.19 How would you interpret a large standard error for a regression coefficient?\n",
        "->A large standard error suggests that the coefficient estimate is less precise, indicating uncertainty in the relationship between the predictor and the outcome.\n",
        "\n",
        "#Q.20 How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "-> Heteroscedasticity can be identified by a funnel shape in residual plots. It is important to address because it violates regression assumptions and can lead to unreliable inference.\n",
        "\n",
        "#Q.21 What does it mean if a Multiple Linear Regression model has a high R-Square but low adjusted R-square?\n",
        "-> It suggests that the model may be overfitted, with too manypredictors that do not contribute significantly to expanding the variance in the dependent variable.\n",
        "\n",
        "#Q.22 Why is it important to scale variables in Multiple Linear Regression?\n",
        "-> Scaling ensures that variables contribute equally to the model, preventing bias due to differences in units or magnitude.\n",
        "\n",
        "#Q.23 What is polynomial regression?\n",
        "-> Polynomial regression is a form of regression analysis where the relationship between the independent variable and the dependent variable is modeled as an nth degree polynomial.\n",
        "\n",
        "#Q.24 How does polynomial regression differ from linear regression?\n",
        "-> Polynomial regression can model can model non- linear relationships, while linear regression assumes a linear relationship.\n",
        "\n",
        "#Q.25 When is polynomial regression used?\n",
        "-> It is used when the relationship between variables is curvilinear and cannot be adequately modeled with a straight line.\n",
        "\n",
        "#Q.26 What is the general equation for polynomial regression?\n",
        "-> The general equation is :\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\dots + \\beta_nX^n + \\epsilon\n",
        "$$\n",
        "In this equation:\n",
        "- Y : Dependent variable\n",
        "- $ \\beta_0 $ : Intercept term\n",
        "- $\\beta_1 , \\beta _2 , \\dots , \\beta_n $: Coefficients for each polynomial term.\n",
        "- X : Independent Variable.\n",
        "- $_2X , _3X , _nX $ : Polynomial terms of the independent variable.\n",
        "- $ \\epsilon $ : Error them\n",
        "\n",
        "#Q.27 Can polynomial regression be applied to multiple variables?\n",
        "-> Yes, polynomial regression can be extended to multiple independent variables.\n",
        "\n",
        "#Q.28 What are the limitations of polynomial regression ?\n",
        "-> Limitations include potential overfitting, increased complexicity, and difficulty in interpreting higher-degree terms.\n",
        "\n",
        "#Q.29 What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "-> Methods include cross-validation, AIC, BIC, and adjusted R-square.\n",
        "\n",
        "#Q.30 Why is visualization important in polynomial regression?\n",
        "-> Visualizations helps in understanding the shape of the relaionship and selecting the appropriate degree of the polynomial.\n",
        "\n",
        "#Q.31 How is polynomial regression implemented in Python?\n",
        "-> Polynomial regression can be implemented using libraries like *scikit - learn* with *PolynomialFeatures* for creating polynomial terms and *LinearRegression* for fitting the model.\n"
      ],
      "metadata": {
        "id": "_fMA4TjJZwc4"
      }
    }
  ]
}